{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are setting three constants SEQ_LEN to know what is the amount of past data we need to predict the next \n",
    "# here it is 60 mins, FUTURE_PERIOD_PREDICT is the data we want to be predicted after FUTURE_PERIOD_PREDICT mins\n",
    "# here it is 3 mins and RATIO_TO_PREDICT is the ratio or the crypto we want to forecast the value of.\n",
    "\n",
    "SEQ_LEN = 60\n",
    "FUTURE_PERIOD_PREDICT = 3\n",
    "RATIO_TO_PREDICT = \"LTC-USD\"\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 64\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(current, future):\n",
    "    if float(future)>float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the preprocess function it drops future col that is of no need to us, then we find the percent change in the values of different ratios of crypto currency prices\n",
    "# we in the way drop the NaN objects and shuffle accordingly, we create seqential_data that takes a seq of rows of SEQ_LEN length and with the target of the 60th row that we \n",
    "# would be predicting then we balance the data using buys and sells list lastly we split the data into the X and i.e. values for input and target\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df = df.drop('future', axis=1)\n",
    "    for col in df.columns:\n",
    "        if col!=\"target\":\n",
    "            df[col] = df[col].pct_change()\n",
    "            df.dropna(inplace = True)\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "    df.dropna(inplace = True)\n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen = SEQ_LEN)\n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])\n",
    "    random.shuffle(sequential_data)\n",
    "    buys = []\n",
    "    sells = []\n",
    "    for seq, target in sequential_data:\n",
    "        if target==0:\n",
    "            sells.append([seq, target])\n",
    "        elif target==1:\n",
    "            buys.append([seq, target])\n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    lower=min(len(buys), len(sells))\n",
    "    buys=buys[:lower]\n",
    "    sells=sells[:lower]\n",
    "    sequential_data = buys+sells\n",
    "    random.shuffle(sequential_data)\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BTC-USD_close  BTC-USD_volume  LTC-USD_close  LTC-USD_volume  \\\n",
      "time                                                                       \n",
      "1528968660    6489.549805        0.587100      96.580002        9.647200   \n",
      "1528968720    6487.379883        7.706374      96.660004      314.387024   \n",
      "1528968780    6479.410156        3.088252      96.570000       77.129799   \n",
      "1528968840    6479.410156        1.404100      96.500000        7.216067   \n",
      "1528968900    6479.979980        0.753000      96.389999      524.539978   \n",
      "\n",
      "            ETH-USD_close  ETH-USD_volume  BCH-USD_close  BCH-USD_volume  \n",
      "time                                                                      \n",
      "1528968660            NaN             NaN     871.719971        5.675361  \n",
      "1528968720      486.01001       26.019083     870.859985       26.856577  \n",
      "1528968780      486.00000        8.449400     870.099976        1.124300  \n",
      "1528968840      485.75000       26.994646     870.789978        1.749862  \n",
      "1528968900      486.00000       77.355759     870.000000        1.680500  \n"
     ]
    }
   ],
   "source": [
    "# here we are reading all the .csv(s) and assigning its columns the names as mentioned in the parameter names\n",
    "# and merging them all to this one dataframe main_df, then we rename the columns to the particular crypto named\n",
    "# attributes to where it belongs and then we set the index for merging later, also we only need the close and volume\n",
    "# so we remove rest of the columns, then we merge them all to main_df.\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "ratios = [\"BTC-USD\",\"LTC-USD\",\"ETH-USD\",\"BCH-USD\"]\n",
    "for ratio in ratios:\n",
    "    dataset = f\"D:/crypto_data/{ratio}.csv\"\n",
    "    df = pd.read_csv(dataset, names=[\"time\", \"low\",\"high\",\"open\",\"close\",\"volume\"])\n",
    "    df.rename(columns={\"close\":f\"{ratio}_close\", \"volume\":f\"{ratio}_volume\"}, inplace=True)\n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]\n",
    "    # print(df.head())\n",
    "    if len(main_df)==0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)\n",
    "\n",
    "print(main_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            LTC-USD_close     future  target\n",
      "time                                        \n",
      "1528968660      96.580002  96.500000       0\n",
      "1528968720      96.660004  96.389999       0\n",
      "1528968780      96.570000  96.519997       0\n",
      "1528968840      96.500000  96.440002       0\n",
      "1528968900      96.389999  96.470001       1\n",
      "1528968960      96.519997  96.400002       0\n",
      "1528969020      96.440002  96.400002       0\n",
      "1528969080      96.470001  96.400002       0\n",
      "1528969140      96.400002  96.400002       0\n",
      "1528969200      96.400002  96.400002       0\n"
     ]
    }
   ],
   "source": [
    "# here we are making a new column future to get a new column target that will be our column to predict\n",
    "# what .shift does is it takes the column shifts it by some number herre -3 that means 1 2 3 times up and assigns all the values in the \n",
    "# column to the new column future, map maps a function and takes function parameters and maps them\n",
    "\n",
    "main_df['future'] = main_df[f\"{RATIO_TO_PREDICT}_close\"].shift(-FUTURE_PERIOD_PREDICT)\n",
    "main_df['target'] = list(map(classify, main_df[f\"{RATIO_TO_PREDICT}_close\"], main_df[\"future\"]))\n",
    "print(main_df[[f\"{RATIO_TO_PREDICT}_close\", \"future\", \"target\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data : 69188, validation 3062\n",
      "don't buys : 34594, buys 34594\n",
      "validation don't buys : 1531, validation buys 1531\n"
     ]
    }
   ],
   "source": [
    "# here we are firstly sorting the df according to time take out the last 5% values as validation then preprocess both the training and testing data and printing some values\n",
    "\n",
    "times = sorted(main_df.index.values)\n",
    "last_5pct = times[-int(0.05*len(times))]\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
    "main_df = main_df[(main_df.index < last_5pct)]\n",
    "\n",
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)\n",
    "\n",
    "print(f\"train data : {len(train_x)}, validation {len(validation_x)}\")\n",
    "print(f\"don't buys : {train_y.count(0)}, buys {train_y.count(1)}\")\n",
    "print(f\"validation don't buys : {validation_y.count(0)}, validation buys {validation_y.count(1)}\")\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "validation_x = np.array(validation_x)\n",
    "validation_y = np.array(validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1082/1082 [==============================] - ETA: 0s - loss: 0.7133 - accuracy: 0.5114\n",
      "Epoch 1: val_accuracy improved from -inf to 0.53919, saving model to model_checkpoints\\RNN_Final-01-0.539.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints\\RNN_Final-01-0.539.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints\\RNN_Final-01-0.539.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082/1082 [==============================] - 386s 351ms/step - loss: 0.7133 - accuracy: 0.5114 - val_loss: 0.6887 - val_accuracy: 0.5392\n",
      "Epoch 2/2\n",
      "1082/1082 [==============================] - ETA: 0s - loss: 0.6889 - accuracy: 0.5411\n",
      "Epoch 2: val_accuracy improved from 0.53919 to 0.56172, saving model to model_checkpoints\\RNN_Final-02-0.562.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints\\RNN_Final-02-0.562.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_checkpoints\\RNN_Final-02-0.562.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082/1082 [==============================] - 354s 327ms/step - loss: 0.6889 - accuracy: 0.5411 - val_loss: 0.6802 - val_accuracy: 0.5617\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, activation=\"tanh\", input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, activation=\"tanh\", input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, activation=\"tanh\", input_shape=(train_x.shape[1:])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer = opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=f'logs/{NAME}')\n",
    "\n",
    "# Specify the directory name for model checkpoints\n",
    "checkpoint_dir = \"model_checkpoints\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Define the filepath for model checkpoints\n",
    "filepath = os.path.join(checkpoint_dir, \"RNN_Final-{epoch:02d}-{val_accuracy:.3f}.model\")\n",
    "\n",
    "# filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"\n",
    "\n",
    "# Create the ModelCheckpoint callback\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"val_accuracy\", verbose=1, save_best_only=True, mode=\"max\")\n",
    "# checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max'))\n",
    "\n",
    "history=model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
